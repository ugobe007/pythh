name: Automated Startup Discovery

on:
  schedule:
    - cron: '0 */12 * * *'
  workflow_dispatch:
    inputs:
      run_type:
        description: 'Type of run'
        required: false
        default: 'full'
        type: choice
        options:
          - full
          - rss_only
          - discovery_only
          - inference_only

env:
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
  VITE_SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  VITE_SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Create logs directory
        run: mkdir -p logs

      - name: Create .env.bak for scraper
        run: |
          cat > .env.bak << EOF
          VITE_SUPABASE_URL=${{ secrets.SUPABASE_URL }}
          VITE_SUPABASE_ANON_KEY=${{ secrets.VITE_SUPABASE_ANON_KEY }}
          SUPABASE_SERVICE_KEY=${{ secrets.SUPABASE_SERVICE_KEY }}
          OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}
          EOF
          echo "âœ… Created .env.bak file"

      - name: Run Automated Scraper
        id: scraper
        continue-on-error: true
        if: ${{ github.event.inputs.run_type != 'inference_only' }}
        run: |
          echo "ðŸš€ Starting SSOT RSS scraper..." | tee logs/scraper.log
          npx tsx scripts/core/ssot-rss-scraper.js 2>&1 | tee -a logs/scraper.log || {
            echo "âš ï¸ Scraper exited with errors, but continuing..."
            echo '{"status": "partial", "scraper_exit_code": '$?'}' > logs/scraper-results.json
            exit 0
          }
          echo '{"status": "success", "timestamp": "'$(date -Iseconds)'"}' > logs/scraper-results.json
        env:
          DEBUG: true

      - name: Run Inference Pipeline
        id: inference
        continue-on-error: true
        run: |
          echo "ðŸ§  Starting ML ontology agent..." | tee -a logs/inference.log
          node scripts/ml-ontology-agent.js 2>&1 | tee -a logs/inference.log || {
            echo "âš ï¸ ML agent exited with errors"
            echo '{"status": "error", "inference_exit_code": '$?'}' > logs/inference-results.json
            exit 0
          }
          echo '{"status": "success", "timestamp": "'$(date -Iseconds)'"}' > logs/inference-results.json
        env:
          DEBUG: true

      - name: Ensure log files exist
        if: always()
        run: |
          # Create placeholder files if scripts didn't create them
          touch logs/scraper.log logs/inference.log
          if [ ! -f logs/scraper-results.json ]; then
            echo '{"status": "not_run", "reason": "Script did not produce output"}' > logs/scraper-results.json
          fi
          if [ ! -f logs/inference-results.json ]; then
            echo '{"status": "not_run", "reason": "Script did not produce output"}' > logs/inference-results.json
          fi
          ls -la logs/

      - name: Upload logs as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: |
            logs/*.log
            logs/*.json
          retention-days: 7
          if-no-files-found: warn

      - name: Report Results
        if: always()
        run: |
          echo "## Pipeline Run Complete" >> $GITHUB_STEP_SUMMARY
          echo "- Run time: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "- Triggered by: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- Run type: ${{ github.event.inputs.run_type || 'full' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f logs/scraper-results.json ]; then
            echo "### Scraper Status" >> $GITHUB_STEP_SUMMARY
            cat logs/scraper-results.json >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          if [ -f logs/inference-results.json ]; then
            echo "### Inference Status" >> $GITHUB_STEP_SUMMARY
            cat logs/inference-results.json >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          if [ -f logs/scraper.log ]; then
            echo "### Recent Scraper Logs" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -30 logs/scraper.log >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

  notify-on-failure:
    needs: scrape
    if: failure()
    runs-on: ubuntu-latest
    steps:
      - name: Log failure
        run: |
          echo "Pipeline failed at $(date)"
          echo "Check the workflow logs for details"
